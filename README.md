# AwesomeLLM
Curated List of Large Language Models
# What is LLMs
Large Language Models (LLMs) are computer programs designed to understand and generate human language. They are based on deep learning techniques, which allow them to learn from vast amounts of data and make predictions based on that knowledge. The development of LLMs is one of the most exciting recent advancements in artificial intelligence and natural language processing.

LLMs are able to generate text that is often indistinguishable from text written by humans. They can be used to write news articles, generate dialogue, create captions for images, and even complete tasks like answering questions and translating languages. LLMs have a wide range of applications, from chatbots to virtual assistants, and they are becoming increasingly important in fields such as journalism, marketing, and customer service.

One of the most important aspects of LLMs is their ability to learn from large amounts of data. They are often trained on massive datasets of text, such as books, articles, and social media posts. By analyzing this data, they are able to learn the patterns and structures of language and use that knowledge to generate new text.

## Large Language Models
- Bloom:BigScience Large Open-science Open-access Multilingual Language Model
  - Model Parameters - 366B
  - [Code](https://huggingface.co/bigscience/bloom)
  - [Paper](https://arxiv.org/abs/2211.05100)
- LLaMA: Open and Efficient Foundation Language Models
  - Model Parameters - 7B, 13B, 33B, 65B
  - [Code](https://github.com/facebookresearch/llama)
  - [Paper](https://arxiv.org/abs/2302.13971)
- OPT (Open Pre-trained Transformers)
  - Model Parameters - 125M, 350M, 1.3B, 2.7B, 13B, 30B, 66B, 175B
  - [Code](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT)
  - [Paper](https://arxiv.org/abs/2205.01068)
## Fine Tuned Models
- Alpaca: Stanford Alpaca: An Instruction-following LLaMA Model
  - Fine tuned on *LLaMA*
  - [Code](https://github.com/tatsu-lab/stanford_alpaca.git)
  - [Paper](https://crfm.stanford.edu/2023/03/13/alpaca.html)
