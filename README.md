# AwesomeLLM
Curated List of Large Language Models
# What is LLMs
Large Language Models (LLMs) are computer programs designed to understand and generate human language. They are based on deep learning techniques, which allow them to learn from vast amounts of data and make predictions based on that knowledge. The development of LLMs is one of the most exciting recent advancements in artificial intelligence and natural language processing.

LLMs are able to generate text that is often indistinguishable from text written by humans. They can be used to write news articles, generate dialogue, create captions for images, and even complete tasks like answering questions and translating languages. LLMs have a wide range of applications, from chatbots to virtual assistants, and they are becoming increasingly important in fields such as journalism, marketing, and customer service.

One of the most important aspects of LLMs is their ability to learn from large amounts of data. They are often trained on massive datasets of text, such as books, articles, and social media posts. By analyzing this data, they are able to learn the patterns and structures of language and use that knowledge to generate new text.

## Large Language Models
- Bloom: BigScience Large Open-science Open-access Multilingual Language Model
  - Model Parameters - 176B
  - [Code](https://huggingface.co/bigscience/bloom)
  - [Paper](https://arxiv.org/abs/2211.05100)
- LLaMA: Open and Efficient Foundation Language Models
  - Model Parameters - 7B, 13B, 33B, 65B
  - [Code](https://github.com/facebookresearch/llama)
  - [Paper](https://arxiv.org/abs/2302.13971)
- OPT (Open Pre-trained Transformers)
  - Model Parameters - 125M, 350M, 1.3B, 2.7B, 13B, 30B, 66B, 175B
  - [Code](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT)
  - [Paper](https://arxiv.org/abs/2205.01068)
- OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization
  - Model parameters - 30B, 175B
  - [Code](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT-IML)
  - [Paper](https://arxiv.org/abs/2212.12017)
## Fine Tuned Models
- Alpaca: Stanford Alpaca: An Instruction-following LLaMA Model
  - Fine tuned on *LLaMA*
  - Application for Dialogue System or ChatGPT alternative
  - [Code](https://github.com/tatsu-lab/stanford_alpaca.git)
  - [Paper](https://crfm.stanford.edu/2023/03/13/alpaca.html)
- Alpaca-lora:
  - Training chatGPT alternative on consumer device gpus
  - [Code](https://github.com/sanjibnarzary/awesome-llm)
- Koala: A Dialogue Model for Academic Research
  - Fine tuned on *LLaMA* 
  - Application for Dialogue System or ChatGPT alternative
  - [Code](https://github.com/young-geng/EasyLM)
  - [Paper](https://bair.berkeley.edu/blog/2023/04/03/koala/)
  - [Demo](https://chat.lmsys.org/?model=koala-13b)

- Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality 
  - Fine tuned on *LLaMA* 
  - Application for Dialogue System or ChatGPT alternative
  - [Code](https://github.com/lm-sys/FastChat)
  - [Paper](https://vicuna.lmsys.org/)
  - [Demo](https://chat.lmsys.org/)
- Xturing: xturing provides fast, efficient and simple fine-tuning of LLMs, such as LLaMA, GPT-J, GPT-2, OPT, Cerebras-GPT, Galactica, and more. By providing an easy-to-use interface for personalizing LLMs to your own data and application, xTuring makes it simple to build and control LLMs. The entire process can be done inside your computer or in your private cloud, ensuring data privacy and security.
  - [Code](https://github.com/stochasticai/xturing)
  - Paper - Not available
  - Demo
  <img src="https://github.com/stochasticai/xturing/raw/main/.github/cli-playground.gif" width="100%" style="margin: 0 1%;"/>

  ## ðŸŒŽ Contributing
As an open source project in a rapidly evolving field, we welcome contributions of all kinds, including new features and better documentation.
